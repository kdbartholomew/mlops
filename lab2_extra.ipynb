{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:34:15.752391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#using conda env mlops_2\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "#also needed to conda install fsspec\n",
    "# conda install -c conda-forge huggingface_hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/mlops_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#text data\n",
    "fraud = pd.read_csv(\"hf://datasets/amitkedia/Financial-Fraud-Dataset/Final_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the \"Fraud\" column to binary\n",
    "fraud[\"Fraud\"] = fraud[\"Fraud\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(fraud[\"Fillings\"], fraud[\"Fraud\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fillings</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nanitem 14 exhibits financial statements repor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item 14 principal accounting fees services mat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item 14 exhibits financial statements schedule...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>item 14 exhibits financial statement schedules...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>item 14 exhibits financial statement schedules...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Fillings  Fraud\n",
       "0  nanitem 14 exhibits financial statements repor...      1\n",
       "1  item 14 principal accounting fees services mat...      0\n",
       "2  item 14 exhibits financial statements schedule...      1\n",
       "3  item 14 exhibits financial statement schedules...      1\n",
       "4  item 14 exhibits financial statement schedules...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# #might be faster than above embedding method:\n",
    "\n",
    "# # Load the Universal Sentence Encoder model\n",
    "# use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# # Generate embeddings for the \"Fillings\" column\n",
    "# embeddings = use_model(fraud[\"Fillings\"].tolist()).numpy()\n",
    "\n",
    "# # Add embeddings to the DataFrame\n",
    "# fraud[\"Embeddings\"] = list(embeddings)\n",
    "\n",
    "# # Check the result\n",
    "# print(fraud[\"Embeddings\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#might be faster than above embedding method:\n",
    "\n",
    "# Load the Universal Sentence Encoder model\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to generate embeddings in batches\n",
    "def generate_embeddings_in_batches(data, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        batch_embeddings = use_model(batch).numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Generate embeddings for the \"Fillings\" column in batches\n",
    "embeddings = generate_embeddings_in_batches(fraud[\"Fillings\"].tolist())\n",
    "\n",
    "# Add embeddings to the DataFrame\n",
    "fraud[\"Embeddings\"] = list(embeddings)\n",
    "\n",
    "# Check the result\n",
    "print(fraud[\"Embeddings\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the file name for the embeddings\n",
    "embeddings_file = 'embeddings.csv'\n",
    "\n",
    "# Check if embeddings already exist\n",
    "try:\n",
    "    # Try to load existing embeddings from CSV\n",
    "    embeddings_df = pd.read_csv(embeddings_file)\n",
    "    print(\"Loaded embeddings from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"{embeddings_file} not found. Please generate embeddings first.\")\n",
    "    # Optionally, you can raise an error or handle it as needed\n",
    "    raise\n",
    "\n",
    "# If you want to add the loaded embeddings to the fraud DataFrame\n",
    "# Assuming 'fraud' DataFrame already exists and has the same number of rows\n",
    "fraud[\"Embeddings\"] = embeddings_df.values.tolist()\n",
    "\n",
    "# Check the result\n",
    "print(fraud[\"Embeddings\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Tokenize the sentences and convert them to a bag-of-words representation\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# # Fit the vectorizer only on the training data (no leakage here)\n",
    "# X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Now transform the test data using the already fitted vectorizer (no leakage here either)\n",
    "# X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# # Convert to DataFrame for easier viewing\n",
    "# X_train_bow_df = pd.DataFrame(X_train_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# print(\"Training Data BOW Representation:\")\n",
    "# print(X_train_bow_df)\n",
    "\n",
    "# X_test_bow_df = pd.DataFrame(X_test_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# print(\"Test Data BOW Representation:\")\n",
    "# print(X_test_bow_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlflow.set_experiment(\"fraud_detection\")\n",
    "# Define models to train\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "# Start MLflow tracking\n",
    "mlflow.start_run()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_bow, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = model.score(X_test_bow, y_test)\n",
    "    \n",
    "    # Log model and metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "    print(f\"{model_name}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper param tuning for xgboost with baysian optimization using hyperopt\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    # Create the model with the suggested hyperparameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=int(params['max_depth']),\n",
    "        min_child_weight=int(params['min_child_weight']),\n",
    "        gamma=params['gamma'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_bow, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_test_bow)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    # Return the accuracy as the objective to minimize (negative because we want to maximize accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'max_depth': hp.randint('max_depth', 3, 10),  # Integer values from 3 to 9\n",
    "    'min_child_weight': hp.randint('min_child_weight', 1, 6),  # Integer values from 1 to 5\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),  # Continuous values from 0 to 0.5\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Continuous values from 0.5 to 1.0\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Continuous values from 0.5 to 1.0\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)  # Continuous values from 0.01 to 0.2\n",
    "}\n",
    "\n",
    "# Create a Trials object to keep track of the results\n",
    "trials = Trials()\n",
    "\n",
    "# Run Hyperopt\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i like xgboost, tuning it to limit overfitting\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"fraud_detection_xgboost\")\n",
    "\n",
    "# For XGBoost tuning - with stronger focus on regularization\n",
    "xgb_param_grid = {\n",
    "    'max_depth': [2, 3, 4],  # Reduced depth to prevent overfitting\n",
    "    'learning_rate': [0.01, 0.05],  # Lower learning rates\n",
    "    'n_estimators': [50, 75],  # Fewer estimators\n",
    "    'subsample': [0.5, 0.7],  # More aggressive subsampling\n",
    "    'colsample_bytree': [0.5, 0.7],  # More aggressive column sampling\n",
    "    'reg_alpha': [0.5, 1, 5],  # L1 regularization\n",
    "    'reg_lambda': [1, 5, 10],  # L2 regularization\n",
    "    'min_child_weight': [3, 5]  # Require more observations per node\n",
    "}\n",
    "\n",
    "# Start tracking\n",
    "mlflow.start_run(run_name=\"XGBoost_Anti_Overfit\")\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "print(\"Starting XGBoost anti-overfitting tuning...\")\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Log best parameters\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    mlflow.log_param(param, value)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_xgb.predict(X_test_bow)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"f1\": f1_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_pred)\n",
    "}\n",
    "\n",
    "# Log metrics\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "# Log model\n",
    "mlflow.sklearn.log_model(best_xgb, \"XGBoost_Anti_Overfit\")\n",
    "\n",
    "# Print results\n",
    "print(f\"XGBoost Anti-Overfitting Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_xgb.feature_importances_\n",
    "top_features_idx = feature_importance.argsort()[-10:]  # Top 10 features\n",
    "top_features_importance = feature_importance[top_features_idx]\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for i, idx in enumerate(top_features_idx):\n",
    "    print(f\"Feature {idx}: {top_features_importance[i]}\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning bert for fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the input\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Create a dataset class\n",
    "class FraudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FraudDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = FraudDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Log the Hugging Face model\n",
    "mlflow.pytorch.log_model(model, \"bert-fraud-detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
